# -*- coding: utf-8 -*-
"""RL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AoDI_92diyV_2mx81Z0BfDtp7V0Zd9v3
"""

!pip install stable-baselines3[extra] gym numpy matplotlib

# Install necessary packages
!pip install gymnasium stable-baselines3[extra] numpy matplotlib

import numpy as np
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import SAC
from stable_baselines3.common.env_checker import check_env
import matplotlib.pyplot as plt

# Heston model simulation function
def heston_simulation(S0, v0, kappa, theta, sigma, rho, r, T, dt, N):
    """
    Simulate stock prices using the Heston model.

    Parameters:
    S0 (float): Initial stock price
    v0 (float): Initial variance
    kappa (float): Speed of reversion to mean
    theta (float): Long-term variance
    sigma (float): Volatility of volatility
    rho (float): Correlation between the Brownian motions of price and variance
    r (float): Risk-free rate
    T (float): Time horizon in years
    dt (float): Time step
    N (int): Number of paths

    Returns:
    np.ndarray: Simulated stock prices of shape (N, int(T/dt))
    """
    num_steps = int(T / dt)
    prices = np.zeros((N, num_steps))
    variances = np.zeros((N, num_steps))
    prices[:, 0] = S0
    variances[:, 0] = v0

    for t in range(1, num_steps):
        dW1 = np.random.normal(0, np.sqrt(dt), size=N)
        dW2 = rho * dW1 + np.sqrt(1 - rho**2) * np.random.normal(0, np.sqrt(dt), size=N)
        variances[:, t] = variances[:, t-1] + kappa * (theta - variances[:, t-1]) * dt + sigma * np.sqrt(variances[:, t-1]) * dW2
        variances[:, t] = np.maximum(variances[:, t], 0)  # Ensure variance is non-negative
        prices[:, t] = prices[:, t-1] * np.exp((r - 0.5 * variances[:, t-1]) * dt + np.sqrt(variances[:, t-1]) * dW1)

    return prices

# Define the custom environment for the RL agent
class PortfolioEnv(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, T=1.0, dt=1/252, initial_portfolio_value=100):
        super(PortfolioEnv, self).__init__()

        # Parameters
        self.T = T
        self.dt = dt
        self.steps = int(T / dt)
        self.initial_portfolio_value = initial_portfolio_value
        self.current_step = 0
        self.time_to_terminal = self.steps
        self.risk_free_rate = 0.055

        # State space: [price_TSLA, price_XOM, portfolio_value, time_to_terminal]
        self.observation_space = spaces.Box(
            low=np.array([0, 0, 0, 0]),
            high=np.array([np.inf, np.inf, np.inf, self.steps]),
            dtype=np.float32
        )

        # Action space: [weight_TSLA, weight_XOM] (2 weights, third is derived as 1 - sum(weights))
        self.action_space = spaces.Box(low=0, high=1, shape=(2,), dtype=np.float32)

        # Heston parameters for TSLA and XOM
        self.heston_params_tsla = {'S0': 14.895333290100098, 'v0': 0.04, 'kappa': 1.5, 'theta': 0.04, 'sigma': 0.2, 'rho': -0.5, 'r': 0.01}
        self.heston_params_xom = {'S0': 52.69083784472894, 'v0': 0.039999945005990946, 'kappa': 1.499999938146619,
                                  'theta': 0.040002536011096795, 'sigma': 0.2000000361535595, 'rho': -0.5000001006619291,
                                  'r': 0.009999849145866864}

        self.seed()

    def seed(self, seed=None):
        np.random.seed(seed)

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.seed(seed)

        self.current_step = 0
        self.time_to_terminal = self.steps
        self.portfolio_value = self.initial_portfolio_value

        # Simulate the price paths
        self.prices_tsla = heston_simulation(**self.heston_params_tsla, T=self.T, dt=self.dt, N=1).flatten()
        self.prices_xom = heston_simulation(**self.heston_params_xom, T=self.T, dt=self.dt, N=1).flatten()

        state = np.array([
            self.prices_tsla[self.current_step],
            self.prices_xom[self.current_step],
            self.portfolio_value,
            self.time_to_terminal
        ], dtype=np.float32)

        return state, {}

    def step(self, action):
        # Ensure weights sum to 1 by including the risk-free asset weight
        weights = np.append(action, 1 - np.sum(action))

        # Compute portfolio value
        self.current_step += 1
        self.time_to_terminal -= 1

        # Check if we are at the terminal state before updating prices
        terminated = self.current_step >= self.steps
        truncated = False  # No truncation logic in this setup

        if terminated:
            state = np.array([
                self.prices_tsla[-1],
                self.prices_xom[-1],
                self.portfolio_value,
                self.time_to_terminal
            ], dtype=np.float32)
            reward = self.portfolio_value
            return state, reward, terminated, truncated, {}

        # Update prices
        price_tsla = self.prices_tsla[self.current_step]
        price_xom = self.prices_xom[self.current_step]

        # Calculate portfolio returns
        returns_tsla = price_tsla / self.prices_tsla[self.current_step - 1] - 1
        returns_xom = price_xom / self.prices_xom[self.current_step - 1] - 1
        returns_rf = self.risk_free_rate * self.dt

        # Update portfolio value
        portfolio_return = weights[0] * returns_tsla + weights[1] * returns_xom + weights[2] * returns_rf
        self.portfolio_value *= (1 + portfolio_return)

        # Calculate reward (Sharpe Ratio)
        reward = (portfolio_return - self.risk_free_rate) / (np.std([returns_tsla, returns_xom, returns_rf]) + 1e-8)

        # Update state
        state = np.array([
            price_tsla,
            price_xom,
            self.portfolio_value,
            self.time_to_terminal
        ], dtype=np.float32)

        return state, reward, terminated, truncated, {}

    def render(self, mode='human'):
        print(f"Step: {self.current_step}, Portfolio Value: {self.portfolio_value:.2f}, Time to Terminal: {self.time_to_terminal}")

    def close(self):
        pass

# Initialize the environment
env = PortfolioEnv()

# Check the environment with Stable Baselines3
check_env(env, warn=True)

# Set the discount factor for this problem
gamma = 0.99  # Suitable discount factor for financial problems

# Initialize the SAC model
model = SAC("MlpPolicy", env, gamma=gamma, verbose=1)

# Train the model
model.learn(total_timesteps=1000 * env.steps)  # 1000 experiments, each with a 1-year simulation

# Save the trained model
model.save("sac_portfolio_model")

# Optionally, load the model
# model = SAC.load("sac_portfolio_model")

# Plotting function to visualize results
def plot_results(prices_tsla, prices_xom, portfolio_values):
    plt.figure(figsize=(12, 6))
    plt.plot(prices_tsla, label='TSLA Prices')
    plt.plot(prices_xom, label='XOM Prices')
    plt.plot(portfolio_values, label='Portfolio Value')
    plt.legend()
    plt.title('Stock Prices and Portfolio Value Over Time')
    plt.xlabel('Time Step')
    plt.ylabel('Value')
    plt.show()

# Run the trained model to evaluate performance
obs, _ = env.reset()
portfolio_values = [env.portfolio_value]

for _ in range(env.steps):
    action, _states = model.predict(obs)
    obs, rewards, terminated, truncated, _ = env.step(action)
    portfolio_values.append(env.portfolio_value)
    env.render()
    if terminated or truncated:
        break

plot_results(env.prices_tsla, env.prices_xom, portfolio_values)

# Install necessary packages
!pip install yfinance gymnasium stable-baselines3 numpy matplotlib pandas scipy

import yfinance as yf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from stable_baselines3 import SAC
from scipy.optimize import minimize

# Fetch real stock price data for TSLA and XOM for the year 2023 using yfinance
start_date = "2023-01-01"
end_date = "2024-08-01"
tickers = ["TSLA", "XOM"]

# Download the data
data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']
data.dropna(inplace=True)

# Load the trained SAC model
model = SAC.load("sac_portfolio_model")

# Initialize variables for tracking portfolio performance
initial_portfolio_value = 100
portfolio_value = initial_portfolio_value
cumulative_rewards_sac = [portfolio_value]

# Initialize variables for the Markowitz optimization
K = 30  # Rolling window size for mean-variance optimization
delta = 0.1  # Risk aversion parameter for Markowitz
cumulative_returns_markowitz = [initial_portfolio_value]

# Define the environment states based on real prices
states = []

# Convert the end date to a timezone-naive datetime
end_date_naive = pd.to_datetime("2024-08-01")

for date in data.index:
    # Ensure 'date' is also timezone-naive
    date_naive = pd.to_datetime(date).replace(tzinfo=None)
    time_to_terminal = (end_date_naive - date_naive).days
    tsla_price = data.loc[date, "TSLA"]
    xom_price = data.loc[date, "XOM"]
    state = np.array([tsla_price, xom_price, portfolio_value, time_to_terminal], dtype=np.float32)
    states.append(state)

# Portfolio optimization function for Markowitz mean-variance optimization
def portfolio_optimization(mu, cov_matrix, delta):
    def objective(weights):
        return - (mu @ weights - delta * weights.T @ cov_matrix @ weights)

    constraints = {'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1}
    init_guess = np.ones(len(mu)) / len(mu)
    bounds = tuple((0, 1) for _ in range(len(mu)))

    result = minimize(objective, init_guess, method='SLSQP', bounds=bounds, constraints=constraints)

    return result.x

# Iterate over each state and make decisions using SAC model and Markowitz optimization
for i in range(len(states) - 1):
    current_state = states[i]
    next_state = states[i + 1]

    # SAC model action prediction
    action, _ = model.predict(current_state)
    weights_sac = np.append(action, 1 - np.sum(action))  # Ensure weights sum to 1

    # Calculate portfolio return using SAC policy
    tsla_return = next_state[0] / current_state[0] - 1
    xom_return = next_state[1] / current_state[1] - 1
    rf_return = 0  # Assume risk-free return is zero for simplicity in the immediate calculation

    portfolio_return_sac = weights_sac[0] * tsla_return + weights_sac[1] * xom_return + weights_sac[2] * rf_return
    portfolio_value *= (1 + portfolio_return_sac)
    cumulative_rewards_sac.append(portfolio_value)

    # Markowitz mean-variance optimization (walk-forward)
    if i >= K:
        window_data = data.iloc[i - K:i]
        returns_df = window_data.pct_change().dropna()

        mu = returns_df.mean().values
        cov_matrix = returns_df.cov().values

        weights_markowitz = portfolio_optimization(mu, cov_matrix, delta)

        # Calculate portfolio return using Markowitz policy
        portfolio_return_markowitz = weights_markowitz[0] * tsla_return + weights_markowitz[1] * xom_return
        cumulative_returns_markowitz.append(cumulative_returns_markowitz[-1] * (1 + portfolio_return_markowitz))
    else:
        cumulative_returns_markowitz.append(cumulative_returns_markowitz[-1])

# Plotting the cumulative returns for both strategies
plt.figure(figsize=(14, 8))
plt.plot(data.index, cumulative_rewards_sac, label='SAC Agent Cumulative Returns', color='blue')
plt.plot(data.index, cumulative_returns_markowitz, label='Markowitz Cumulative Returns (Delta = 1)', color='red')
plt.title('Cumulative Returns: SAC Agent vs. Markowitz Portfolio')
plt.xlabel('Date')
plt.ylabel('Cumulative Returns')
plt.legend()
plt.grid(True)
plt.show()

# Install necessary packages
!pip install gymnasium stable-baselines3[extra] numpy matplotlib

import numpy as np
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import SAC
from stable_baselines3.common.env_checker import check_env
import matplotlib.pyplot as plt
import torch

# Heston model simulation function
def heston_simulation(S0, v0, kappa, theta, sigma, rho, r, T, dt, N):
    num_steps = int(T / dt)
    prices = np.zeros((N, num_steps))
    variances = np.zeros((N, num_steps))
    prices[:, 0] = S0
    variances[:, 0] = v0

    for t in range(1, num_steps):
        dW1 = np.random.normal(0, np.sqrt(dt), size=N)
        dW2 = rho * dW1 + np.sqrt(1 - rho**2) * np.random.normal(0, np.sqrt(dt), size=N)
        variances[:, t] = variances[:, t-1] + kappa * (theta - variances[:, t-1]) * dt + sigma * np.sqrt(variances[:, t-1]) * dW2
        variances[:, t] = np.maximum(variances[:, t], 0)  # Ensure variance is non-negative
        prices[:, t] = prices[:, t-1] * np.exp((r - 0.5 * variances[:, t-1]) * dt + np.sqrt(variances[:, t-1]) * dW1)

    return prices

# Define the custom environment for the RL agent
class PortfolioEnv(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, T=1.0, dt=1/252, initial_portfolio_value=100, reg_coeff=0.01):
        super(PortfolioEnv, self).__init__()

        # Parameters
        self.T = T
        self.dt = dt
        self.steps = int(T / dt)
        self.initial_portfolio_value = initial_portfolio_value
        self.current_step = 0
        self.time_to_terminal = self.steps
        self.risk_free_rate = 0.055
        self.reg_coeff = reg_coeff  # Regularization coefficient for reward shaping

        # State space: [price_TSLA, price_XOM, portfolio_value, time_to_terminal]
        self.observation_space = spaces.Box(
            low=np.array([0, 0, 0, 0]),
            high=np.array([np.inf, np.inf, np.inf, self.steps]),
            dtype=np.float32
        )

        # Action space: [weight_TSLA, weight_XOM] (2 weights, third is derived as 1 - sum(weights))
        self.action_space = spaces.Box(low=0, high=1, shape=(2,), dtype=np.float32)

        # Heston parameters for TSLA and XOM
        self.heston_params_tsla = {'S0': 14.895333290100098, 'v0': 0.04, 'kappa': 1.5, 'theta': 0.04, 'sigma': 0.2, 'rho': -0.5, 'r': 0.01}
        self.heston_params_xom = {'S0': 52.69083784472894, 'v0': 0.039999945005990946, 'kappa': 1.499999938146619,
                                  'theta': 0.040002536011096795, 'sigma': 0.2000000361535595, 'rho': -0.5000001006619291,
                                  'r': 0.009999849145866864}

        self.seed()

    def seed(self, seed=None):
        np.random.seed(seed)

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.seed(seed)

        self.current_step = 0
        self.time_to_terminal = self.steps
        self.portfolio_value = self.initial_portfolio_value

        # Simulate the price paths
        self.prices_tsla = heston_simulation(**self.heston_params_tsla, T=self.T, dt=self.dt, N=1).flatten()
        self.prices_xom = heston_simulation(**self.heston_params_xom, T=self.T, dt=self.dt, N=1).flatten()

        state = np.array([
            self.prices_tsla[self.current_step],
            self.prices_xom[self.current_step],
            self.portfolio_value,
            self.time_to_terminal
        ], dtype=np.float32)

        return state, {}

    def step(self, action):
        # Ensure weights sum to 1 by including the risk-free asset weight
        weights = np.append(action, 1 - np.sum(action))

        # Compute portfolio value
        self.current_step += 1
        self.time_to_terminal -= 1

        # Check if we are at the terminal state before updating prices
        terminated = self.current_step >= self.steps
        truncated = False  # No truncation logic in this setup

        if terminated:
            state = np.array([
                self.prices_tsla[-1],
                self.prices_xom[-1],
                self.portfolio_value,
                self.time_to_terminal
            ], dtype=np.float32)
            reward = self.portfolio_value
            return state, reward, terminated, truncated, {}

        # Update prices
        price_tsla = self.prices_tsla[self.current_step]
        price_xom = self.prices_xom[self.current_step]

        # Calculate portfolio returns
        returns_tsla = price_tsla / self.prices_tsla[self.current_step - 1] - 1
        returns_xom = price_xom / self.prices_xom[self.current_step - 1] - 1
        returns_rf = self.risk_free_rate * self.dt

        # Update portfolio value
        portfolio_return = weights[0] * returns_tsla + weights[1] * returns_xom + weights[2] * returns_rf
        self.portfolio_value *= (1 + portfolio_return)

        # Calculate reward (Sharpe Ratio) and normalize rewards
        reward = (portfolio_return - self.risk_free_rate) / (np.std([returns_tsla, returns_xom, returns_rf]) + 1e-8)
        reward /= 10  # Normalize rewards to keep within a manageable range

        # Regularization: Penalize large changes in weights (to avoid excessive trading)
        penalty = self.reg_coeff * np.sum(np.square(action - np.array([0.5, 0.5])))  # Example penalty calculation
        reward -= penalty

        # Update state
        state = np.array([
            price_tsla,
            price_xom,
            self.portfolio_value,
            self.time_to_terminal
        ], dtype=np.float32)

        return state, reward, terminated, truncated, {}

    def render(self, mode='human'):
        print(f"Step: {self.current_step}, Portfolio Value: {self.portfolio_value:.2f}, Time to Terminal: {self.time_to_terminal}")

    def close(self):
        pass

# Initialize the environment
env = PortfolioEnv()

# Check the environment with Stable Baselines3
check_env(env, warn=True)

# Set the discount factor for this problem
gamma = 0.99  # Suitable discount factor for financial problems

# Further reduce the learning rate for better stability
learning_rate = 0.00001  # Further reduced learning rate

# Manually control entropy coefficient
ent_coef = 'auto_0.1'  # Start with a higher exploration rate and decay more aggressively

# Initialize the SAC model with L2 regularization (weight_decay)
policy_kwargs = dict(optimizer_kwargs=dict(weight_decay=1e-5))  # Adding L2 regularization

# Initialize the SAC model with adjusted parameters
model = SAC("MlpPolicy", env, gamma=gamma, learning_rate=learning_rate, ent_coef=ent_coef, policy_kwargs=policy_kwargs, verbose=1)

# Train the model
model.learn(total_timesteps=2000 * env.steps)  # Increase the number of experiments to allow for better convergence

# Save the trained model
model.save("sac_portfolio_model")

# Optionally, load the model
# model = SAC.load("sac_portfolio_model")

# Plotting function to visualize results
def plot_results(prices_tsla, prices_xom, portfolio_values):
    plt.figure(figsize=(12, 6))
    plt.plot(prices_tsla, label='TSLA Prices')
    plt.plot(prices_xom, label='XOM Prices')
    plt.plot(portfolio_values, label='Portfolio Value')
    plt.legend()
    plt.title('Stock Prices and Portfolio Value Over Time')
    plt.xlabel('Time Step')
    plt.ylabel('Value')
    plt.show()

# Run the trained model to evaluate performance
obs, _ = env.reset()
portfolio_values = [env.portfolio_value]

for _ in range(env.steps):
    action, _states = model.predict(obs)
    obs, rewards, terminated, truncated, _ = env.step(action)
    portfolio_values.append(env.portfolio_value)
    env.render()
    if terminated or truncated:
        break

plot_results(env.prices_tsla, env.prices_xom, portfolio_values)

!pip install gymnasium stable-baselines3[extra] numpy matplotlib pandas scipy

# Install necessary packages
!pip install gymnasium stable-baselines3[extra] numpy matplotlib pandas scipy

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.optimize import minimize
from scipy.stats import chi2
from stable_baselines3 import SAC

# Heston model simulation function
def heston_simulation(S0, v0, kappa, theta, sigma, rho, r, T, dt, N):
    num_steps = int(T / dt)
    prices = np.zeros((N, num_steps))
    variances = np.zeros((N, num_steps))
    prices[:, 0] = S0
    variances[:, 0] = v0

    for t in range(1, num_steps):
        dW1 = np.random.normal(0, np.sqrt(dt), size=N)
        dW2 = rho * dW1 + np.sqrt(1 - rho**2) * np.random.normal(0, np.sqrt(dt), size=N)
        variances[:, t] = variances[:, t-1] + kappa * (theta - variances[:, t-1]) * dt + sigma * np.sqrt(variances[:, t-1]) * dW2
        variances[:, t] = np.maximum(variances[:, t], 0)  # Ensure variance is non-negative
        prices[:, t] = prices[:, t-1] * np.exp((r - 0.5 * variances[:, t-1]) * dt + np.sqrt(variances[:, t-1]) * dW1)

    return prices

# Function to run a single experiment
def run_experiment(delta, initial_portfolio_value=100):
    # Generate data using Heston parameters for TSLA and XOM
    T = 1.0  # 1 year
    dt = 1/252  # Daily steps
    N = 1  # One simulation path

    heston_params_tsla = {'S0': 14.895333290100098, 'v0': 0.04, 'kappa': 1.5, 'theta': 0.04, 'sigma': 0.2, 'rho': -0.5, 'r': 0.01}
    heston_params_xom = {'S0': 52.69083784472894, 'v0': 0.039999945005990946, 'kappa': 1.499999938146619,
                         'theta': 0.040002536011096795, 'sigma': 0.2000000361535595, 'rho': -0.5000001006619291,
                         'r': 0.009999849145866864}

    # Simulate price paths
    prices_tsla = heston_simulation(**heston_params_tsla, T=T, dt=dt, N=N).flatten()
    prices_xom = heston_simulation(**heston_params_xom, T=T, dt=dt, N=N).flatten()

    # Create DataFrame of returns
    returns_df = pd.DataFrame({
        'TSLA_Returns': np.diff(prices_tsla) / prices_tsla[:-1],
        'XOM_Returns': np.diff(prices_xom) / prices_xom[:-1]
    })

    num_assets = len(returns_df.columns)

    # Estimating Sigma_C (Covariance of the Covariance Matrix)
    def bootstrap_cov_matrix(returns_df, n_bootstraps=1000):
        n_assets = returns_df.shape[1]
        n_elements = int(n_assets * (n_assets + 1) / 2)
        bootstrap_cov_elements = []

        for _ in range(n_bootstraps):
            resampled_returns = returns_df.sample(n=len(returns_df), replace=True)
            cov_matrix = resampled_returns.cov().values
            cov_elements = cov_matrix[np.triu_indices(n_assets)]
            bootstrap_cov_elements.append(cov_elements)

        bootstrap_cov_df = pd.DataFrame(bootstrap_cov_elements)
        Sigma_C = bootstrap_cov_df.cov().values

        return Sigma_C

    Sigma_C = bootstrap_cov_matrix(returns_df)

    degrees_of_freedom = (num_assets * (num_assets + 1)) // 2
    confidence_level = 0.95
    delta_C = chi2.ppf(confidence_level, df=degrees_of_freedom)

    regularization = 1e-4
    K = 30  # Window size for rolling optimization

    # Optimization function for robust mean-variance
    def robust_portfolio_optimization_cov(x, mu, C, Sigma_C, delta_C):
        x_C_x = np.array([x[i] * x[j] for i in range(num_assets) for j in range(i, num_assets)])
        sqrt_term = np.sqrt(delta_C) * np.sqrt(np.dot(x_C_x.T, np.dot(Sigma_C, x_C_x)))
        return -(mu.T @ x - sqrt_term)

    def non_robust_portfolio_optimization(x, C):
        return x.T @ C @ x

    robust_portfolio_returns = [initial_portfolio_value]
    non_robust_portfolio_returns = [initial_portfolio_value]

    portfolio_value_robust_sac = initial_portfolio_value
    portfolio_value_non_robust_sac = initial_portfolio_value

    cumulative_returns_robust_sac = [portfolio_value_robust_sac]
    cumulative_returns_non_robust_sac = [portfolio_value_non_robust_sac]

    # Load the trained SAC models
    robust_sac_model = SAC.load("robust_sac.zip")
    non_robust_sac_model = SAC.load("non_robust_sac.zip")

    for t in range(K, len(returns_df) - 1):
        window_returns = returns_df.iloc[t-K:t]
        mu = window_returns.mean().values
        C = window_returns.cov().values + regularization * np.eye(num_assets)

        init_guess = np.ones(num_assets) / num_assets
        constraints = {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}
        bounds = tuple((0, 1) for _ in range(num_assets))

        result_robust = minimize(robust_portfolio_optimization_cov, init_guess,
                                 args=(mu, C, Sigma_C, delta_C),
                                 method='SLSQP', bounds=bounds, constraints=constraints)

        result_non_robust = minimize(non_robust_portfolio_optimization, init_guess,
                                     args=(C,),
                                     method='SLSQP', bounds=bounds, constraints=constraints)

        if result_robust.success:
            optimal_weights_robust = result_robust.x
            next_return_robust = returns_df.iloc[t + 1].values @ optimal_weights_robust
            portfolio_value_robust = robust_portfolio_returns[-1] * (1 + next_return_robust)
        else:
            portfolio_value_robust = robust_portfolio_returns[-1]

        if result_non_robust.success:
            optimal_weights_non_robust = result_non_robust.x
            next_return_non_robust = returns_df.iloc[t + 1].values @ optimal_weights_non_robust
            portfolio_value_non_robust = non_robust_portfolio_returns[-1] * (1 + next_return_non_robust)
        else:
            portfolio_value_non_robust = non_robust_portfolio_returns[-1]

        robust_portfolio_returns.append(portfolio_value_robust)
        non_robust_portfolio_returns.append(portfolio_value_non_robust)

        state = np.array([
            prices_tsla[t],
            prices_xom[t],
            portfolio_value_robust_sac,
            len(returns_df) - t
        ], dtype=np.float32)

        action_robust, _ = robust_sac_model.predict(state)
        weights_robust_sac = np.append(action_robust, 1 - np.sum(action_robust))
        portfolio_return_robust_sac = weights_robust_sac[0] * (prices_tsla[t + 1] / prices_tsla[t] - 1) + \
                                      weights_robust_sac[1] * (prices_xom[t + 1] / prices_xom[t] - 1)
        portfolio_value_robust_sac *= (1 + portfolio_return_robust_sac)
        cumulative_returns_robust_sac.append(portfolio_value_robust_sac)

        action_non_robust, _ = non_robust_sac_model.predict(state)
        weights_non_robust_sac = np.append(action_non_robust, 1 - np.sum(action_non_robust))
        portfolio_return_non_robust_sac = weights_non_robust_sac[0] * (prices_tsla[t + 1] / prices_tsla[t] - 1) + \
                                          weights_non_robust_sac[1] * (prices_xom[t + 1] / prices_xom[t] - 1)
        portfolio_value_non_robust_sac *= (1 + portfolio_return_non_robust_sac)
        cumulative_returns_non_robust_sac.append(portfolio_value_non_robust_sac)

    # Ensure matching lengths for plotting
    time_index = returns_df.index[K+1:len(returns_df)]
    cumulative_returns_robust_mv = np.array(robust_portfolio_returns)[:len(time_index)]
    cumulative_returns_non_robust_mv = np.array(non_robust_portfolio_returns)[:len(time_index)]
    cumulative_returns_robust_sac = np.array(cumulative_returns_robust_sac)[:len(time_index)]
    cumulative_returns_non_robust_sac = np.array(cumulative_returns_non_robust_sac)[:len(time_index)]

    return (cumulative_returns_robust_mv, cumulative_returns_non_robust_mv,
            cumulative_returns_robust_sac, cumulative_returns_non_robust_sac)

# Set delta values and number of experiments
delta_values = [0.1, 0.5, 1, 10]
num_experiments = 100

# Initialize dictionaries to store results
mean_cumulative_returns = {
    'robust_mv': {delta: [] for delta in delta_values},
    'non_robust_mv': {delta: [] for delta in delta_values},
    'robust_sac': {delta: [] for delta in delta_values},
    'non_robust_sac': {delta: [] for delta in delta_values}
}

# Run experiments
for delta in delta_values:
    robust_mv_results = []
    non_robust_mv_results = []
    robust_sac_results = []
    non_robust_sac_results = []

    for _ in range(num_experiments):
        robust_mv, non_robust_mv, robust_sac, non_robust_sac = run_experiment(delta)
        robust_mv_results.append(robust_mv)
        non_robust_mv_results.append(non_robust_mv)
        robust_sac_results.append(robust_sac)
        non_robust_sac_results.append(non_robust_sac)

    # Calculate mean cumulative returns for each model
    mean_cumulative_returns['robust_mv'][delta] = np.mean(robust_mv_results, axis=0)
    mean_cumulative_returns['non_robust_mv'][delta] = np.mean(non_robust_mv_results, axis=0)
    mean_cumulative_returns['robust_sac'][delta] = np.mean(robust_sac_results, axis=0)
    mean_cumulative_returns['non_robust_sac'][delta] = np.mean(non_robust_sac_results, axis=0)

# Plotting results
fig, axes = plt.subplots(2, 2, figsize=(18, 14))
axes = axes.flatten()

for idx, delta in enumerate(delta_values):
    ax = axes[idx]
    time_index = returns_df.index[K+1:len(returns_df)]

    ax.plot(time_index, mean_cumulative_returns['robust_mv'][delta], label='Robust Mean-Var', color='red')
    ax.plot(time_index, mean_cumulative_returns['non_robust_mv'][delta], label='Non-Robust Mean-Var', color='blue')
    ax.plot(time_index, mean_cumulative_returns['robust_sac'][delta], label='Robust SAC', color='green')
    ax.plot(time_index, mean_cumulative_returns['non_robust_sac'][delta], label='Non-Robust SAC', color='purple')

    ax.set_title(f'Delta = {delta}')
    ax.set_xlabel('Date')
    ax.set_ylabel('Mean Cumulative Returns')
    ax.legend()
    ax.grid(True)

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.suptitle('Mean Cumulative Portfolio Returns: Robust vs Non-Robust Optimization (Covariance Uncertainty) and SAC Models', fontsize=16)
plt.show()

