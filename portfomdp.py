# -*- coding: utf-8 -*-
"""PortfoMDP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-KmTRhqMeVQykFq0u7BQN10nIGOm-12d
"""

# Install necessary packages
!pip install gymnasium stable-baselines3[extra] numpy matplotlib pandas scipy

import numpy as np
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import SAC
from stable_baselines3.common.env_checker import check_env
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# Heston model simulation function
def heston_simulation(S0, v0, kappa, theta, sigma, rho, r, T, dt, N):
    """
    Simulate stock prices using the Heston model.
    """
    num_steps = int(T / dt)
    prices = np.zeros((N, num_steps))
    variances = np.zeros((N, num_steps))
    prices[:, 0] = S0
    variances[:, 0] = v0

    for t in range(1, num_steps):
        dW1 = np.random.normal(0, np.sqrt(dt), size=N)
        dW2 = rho * dW1 + np.sqrt(1 - rho**2) * np.random.normal(0, np.sqrt(dt), size=N)
        variances[:, t] = variances[:, t-1] + kappa * (theta - variances[:, t-1]) * dt + sigma * np.sqrt(variances[:, t-1]) * dW2
        variances[:, t] = np.maximum(variances[:, t], 0)  # Ensure variance is non-negative
        prices[:, t] = prices[:, t-1] * np.exp((r - 0.5 * variances[:, t-1]) * dt + np.sqrt(variances[:, t-1]) * dW1)

    return prices

# Define the custom environment for the RL agent
class PortfolioEnv(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, T=1.0, dt=1/252, initial_portfolio_value=100):
        super(PortfolioEnv, self).__init__()

        # Parameters
        self.T = T
        self.dt = dt
        self.steps = int(T / dt)
        self.initial_portfolio_value = initial_portfolio_value
        self.current_step = 30  # Start after collecting initial window for mean/volatility
        self.risk_free_rate = 0.045 / 252  # Daily risk-free rate approximation
        self.discount_factor = (1 - self.risk_free_rate)

        # Updated state space to match the correct observation vector length
        # State space: [current_prices (2), current_weights (3), portfolio_value (1), mean_prices (2), volatility (2), tau (1)]
        self.observation_space = spaces.Box(
            low=np.array([0, 0, 0, 0, 0, 0, -np.inf, 0, 0, 0, 0]),
            high=np.array([np.inf, np.inf, 1, 1, 1, np.inf, np.inf, np.inf, np.inf, np.inf, self.steps]),
            dtype=np.float32
        )

        # Action space: portfolio weights for TSLA and Nvidia, third weight is derived (risk-free)
        self.action_space = spaces.Box(low=0, high=1, shape=(2,), dtype=np.float32)

        # Heston parameters for TSLA and Nvidia
        self.heston_params_tsla = {'S0': 14.895333290100098, 'v0': 0.5389647040148708, 'kappa': 2.9210956956390786, 'theta': 0.22837916745978026, 'sigma': 0.40481651452195677, 'rho': -0.07362857586873965, 'r': 0.01}
        self.heston_params_Nvidia = {'S0': 52.69083784472894, 'v0': 0.5208742572323116, 'kappa': 0.49265002435163474,
                                  'theta': 0.41686951612919876, 'sigma': 0.32390578896758004, 'rho': 0.4888657113235263,
                                  'r': 0.043}

        self.reset()

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.current_step = 30
        self.portfolio_value = self.initial_portfolio_value

        # Simulate the price paths
        self.prices_tsla = heston_simulation(**self.heston_params_tsla, T=self.T, dt=self.dt, N=1).flatten()
        self.prices_Nvidia = heston_simulation(**self.heston_params_Nvidia, T=self.T, dt=self.dt, N=1).flatten()

        self.weights = np.array([1/3, 1/3, 1/3])  # Start with equal weights
        self.tau = self.steps - self.current_step  # Time to end of episode

        # Calculate initial mean and volatility for the last 30 steps
        self.mean_tsla = np.mean(self.prices_tsla[:self.current_step])
        self.mean_Nvidia = np.mean(self.prices_Nvidia[:self.current_step])
        self.volatility_tsla = np.std(self.prices_tsla[:self.current_step])
        self.volatility_Nvidia = np.std(self.prices_Nvidia[:self.current_step])

        return self._get_obs(), {}

    def _get_obs(self):
        # Correct the observation vector to match the observation space definition
        return np.array([
            self.prices_tsla[self.current_step],
            self.prices_Nvidia[self.current_step],
            *self.weights,
            self.portfolio_value,
            self.mean_tsla,
            self.mean_Nvidia,
            self.volatility_tsla,
            self.volatility_Nvidia,
            self.tau
        ], dtype=np.float32)

    def step(self, action):
        # Ensure weights sum to 1 by including the risk-free asset weight
        weights = np.append(action, 1 - np.sum(action))

        # Clip weights to ensure they are within [0, 1] range and sum to 1
        weights = np.clip(weights, 0, 1)
        weights = weights / weights.sum()

        # Update portfolio
        self.current_step += 1
        self.tau -= 1  # Decrease the time to terminal
        if self.current_step >= (self.steps - 1):
            terminated = True
            truncated = False
            reward = 0
            return self._get_obs(), reward, terminated, truncated, {}

        terminated = self.current_step >= self.steps - 1  # Stop before we run out of data
        truncated = False

        # Calculate returns
        returns_tsla = (self.prices_tsla[self.current_step] - self.prices_tsla[self.current_step - 1]) / self.prices_tsla[self.current_step - 1]
        returns_Nvidia = (self.prices_Nvidia[self.current_step] - self.prices_Nvidia[self.current_step - 1]) / self.prices_Nvidia[self.current_step - 1]
        returns_rf = self.risk_free_rate

        # Portfolio return
        portfolio_return = weights[0] * returns_tsla + weights[1] * returns_Nvidia + weights[2] * returns_rf
        self.portfolio_value *= (1 + portfolio_return)

        # Update moving averages and volatilities
        self.mean_tsla = np.mean(self.prices_tsla[self.current_step-30:self.current_step])
        self.mean_Nvidia = np.mean(self.prices_Nvidia[self.current_step-30:self.current_step])
        self.volatility_tsla = np.std(self.prices_tsla[self.current_step-30:self.current_step])
        self.volatility_Nvidia = np.std(self.prices_Nvidia[self.current_step-30:self.current_step])

        # Update state
        self.weights = weights
        state = self._get_obs()

        # Reward calculation (Sharpe Ratio)
        excess_return = portfolio_return - self.risk_free_rate
        risk = np.std([returns_tsla, returns_Nvidia, returns_rf])
        sharpe_ratio = excess_return / (risk + 1e-8)
        reward = sharpe_ratio

        return state, reward, terminated, truncated, {}

    def render(self, mode='human'):
        print(f"Step: {self.current_step}, Portfolio Value: {self.portfolio_value:.2f}, Time to End: {self.tau}")

    def close(self):
        pass

# Initialize the environment
env = PortfolioEnv()

# Check the environment with Stable Baselines3
check_env(env, warn=True)

# Initialize the SAC model with the new environment
model = SAC("MlpPolicy", env, gamma=env.discount_factor, verbose=1)

# Train the model
model.learn(total_timesteps=100000)

# Save the trained model
model.save("NvidiaTeslaUnregulized")

# Run the trained model to evaluate performance
obs, _ = env.reset()
portfolio_values = [env.portfolio_value]

for _ in range(env.steps - env.current_step):  # Adjust the loop to start from current_step
    action, _states = model.predict(obs)
    obs, rewards, terminated, truncated, _ = env.step(action)
    portfolio_values.append(env.portfolio_value)
    env.render()
    if terminated or truncated:
        break

# Plotting function to visualize results
plt.figure(figsize=(12, 6))
plt.plot(portfolio_values, label='Portfolio Value')
plt.legend()
plt.title('Portfolio Value Over Time')
plt.xlabel('Time Step')
plt.ylabel('Value')
plt.show()

# Install necessary packages
!pip install gymnasium stable-baselines3[extra] numpy matplotlib pandas scipy

import numpy as np
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import SAC
from stable_baselines3.common.env_checker import check_env
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# Heston model simulation function
def heston_simulation(S0, v0, kappa, theta, sigma, rho, r, T, dt, N):
    """
    Simulate stock prices using the Heston model.
    """
    num_steps = int(T / dt)
    prices = np.zeros((N, num_steps))
    variances = np.zeros((N, num_steps))
    prices[:, 0] = S0
    variances[:, 0] = v0

    for t in range(1, num_steps):
        dW1 = np.random.normal(0, np.sqrt(dt), size=N)
        dW2 = rho * dW1 + np.sqrt(1 - rho**2) * np.random.normal(0, np.sqrt(dt), size=N)
        variances[:, t] = variances[:, t-1] + kappa * (theta - variances[:, t-1]) * dt + sigma * np.sqrt(variances[:, t-1]) * dW2
        variances[:, t] = np.maximum(variances[:, t], 0)  # Ensure variance is non-negative
        prices[:, t] = prices[:, t-1] * np.exp((r - 0.5 * variances[:, t-1]) * dt + np.sqrt(variances[:, t-1]) * dW1)

    return prices

# Define the custom environment for the RL agent
class PortfolioEnv(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, T=1.0, dt=1/252, initial_portfolio_value=100):
        super(PortfolioEnv, self).__init__()

        # Parameters
        self.T = T
        self.dt = dt
        self.steps = int(T / dt)
        self.initial_portfolio_value = initial_portfolio_value
        self.current_step = 30  # Start after collecting initial window for mean/volatility
        self.risk_free_rate = 0.045 / 252  # Daily risk-free rate approximation
        self.discount_factor = (1 - self.risk_free_rate)

        # Regularization parameters
        self.Sigma = np.array([[0.000523, 0.000275], [0.000275, 0.000878]])  # Covariance matrix for Mahalanobis norm
        self.delta_r = 0.06091841496941713  # Radius delta_r

        # Updated state space to match the correct observation vector length
        # State space: [current_prices (2), current_weights (3), portfolio_value (1), mean_prices (2), volatility (2), tau (1)]
        self.observation_space = spaces.Box(
            low=np.array([0, 0, 0, 0, 0, 0, -np.inf, 0, 0, 0, 0]),
            high=np.array([np.inf, np.inf, 1, 1, 1, np.inf, np.inf, np.inf, np.inf, np.inf, self.steps]),
            dtype=np.float32
        )

        # Action space: portfolio weights for TSLA and Nvidia, third weight is derived (risk-free)
        self.action_space = spaces.Box(low=0, high=1, shape=(2,), dtype=np.float32)

        # Heston parameters for TSLA and Nvidia
        self.heston_params_tsla = {'S0': 14.895333290100098, 'v0': 0.5389647040148708, 'kappa': 2.9210956956390786, 'theta': 0.22837916745978026, 'sigma': 0.40481651452195677, 'rho': -0.07362857586873965, 'r': 0.01}
        self.heston_params_Nvidia = {'S0': 52.69083784472894, 'v0': 0.5208742572323116, 'kappa': 0.49265002435163474,
                                  'theta': 0.41686951612919876, 'sigma': 0.32390578896758004, 'rho': 0.4888657113235263,
                                  'r': 0.043}

        self.reset()

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.current_step = 30
        self.portfolio_value = self.initial_portfolio_value

        # Simulate the price paths
        self.prices_tsla = heston_simulation(**self.heston_params_tsla, T=self.T, dt=self.dt, N=1).flatten()
        self.prices_Nvidia = heston_simulation(**self.heston_params_Nvidia, T=self.T, dt=self.dt, N=1).flatten()

        self.weights = np.array([1/3, 1/3, 1/3])  # Start with equal weights
        self.tau = self.steps - self.current_step  # Time to end of episode

        # Calculate initial mean and volatility for the last 30 steps
        self.mean_tsla = np.mean(self.prices_tsla[:self.current_step])
        self.mean_Nvidia = np.mean(self.prices_Nvidia[:self.current_step])
        self.volatility_tsla = np.std(self.prices_tsla[:self.current_step])
        self.volatility_Nvidia = np.std(self.prices_Nvidia[:self.current_step])

        return self._get_obs(), {}

    def _get_obs(self):
        # Correct the observation vector to match the observation space definition
        return np.array([
            self.prices_tsla[self.current_step],
            self.prices_Nvidia[self.current_step],
            *self.weights,
            self.portfolio_value,
            self.mean_tsla,
            self.mean_Nvidia,
            self.volatility_tsla,
            self.volatility_Nvidia,
            self.tau
        ], dtype=np.float32)

    def mahalanobis_norm(self, action):
        """
        Compute the Mahalanobis norm of the policy (action) using the covariance matrix Sigma.
        """
        action = np.array(action).reshape(-1, 1)
        norm = np.sqrt(action.T @ np.linalg.inv(self.Sigma) @ action)
        return norm[0][0]

    def step(self, action):
        # Ensure weights sum to 1 by including the risk-free asset weight
        weights = np.append(action, 1 - np.sum(action))

        # Clip weights to ensure they are within [0, 1] range and sum to 1
        weights = np.clip(weights, 0, 1)
        weights = weights / weights.sum()

        # Update portfolio
        self.current_step += 1
        self.tau -= 1  # Decrease the time to terminal
        if self.current_step >= (self.steps - 1):
            terminated = True
            truncated = False
            reward = 0
            return self._get_obs(), reward, terminated, truncated, {}

        terminated = self.current_step >= self.steps - 1  # Stop before we run out of data
        truncated = False

        # Calculate returns
        returns_tsla = (self.prices_tsla[self.current_step] - self.prices_tsla[self.current_step - 1]) / self.prices_tsla[self.current_step - 1]
        returns_Nvidia = (self.prices_Nvidia[self.current_step] - self.prices_Nvidia[self.current_step - 1]) / self.prices_Nvidia[self.current_step - 1]
        returns_rf = self.risk_free_rate

        # Portfolio return
        portfolio_return = weights[0] * returns_tsla + weights[1] * returns_Nvidia + weights[2] * returns_rf
        self.portfolio_value *= (1 + portfolio_return)

        # Update moving averages and volatilities
        self.mean_tsla = np.mean(self.prices_tsla[self.current_step-30:self.current_step])
        self.mean_Nvidia = np.mean(self.prices_Nvidia[self.current_step-30:self.current_step])
        self.volatility_tsla = np.std(self.prices_tsla[self.current_step-30:self.current_step])
        self.volatility_Nvidia = np.std(self.prices_Nvidia[self.current_step-30:self.current_step])

        # Update state
        self.weights = weights
        state = self._get_obs()

        # Reward calculation (Sharpe Ratio)
        excess_return = portfolio_return - self.risk_free_rate
        risk = np.std([returns_tsla, returns_Nvidia, returns_rf])
        sharpe_ratio = excess_return / (risk + 1e-8)

        # Regularization penalty: -sqrt(delta_r) * || pi_s ||_Sigma
        pi_s_norm = self.mahalanobis_norm(action)
        penalty = -np.sqrt(self.delta_r) * pi_s_norm

        # Final reward with penalty
        reward = sharpe_ratio + penalty

        return state, reward, terminated, truncated, {}

    def render(self, mode='human'):
        print(f"Step: {self.current_step}, Portfolio Value: {self.portfolio_value:.2f}, Time to End: {self.tau}")

    def close(self):
        pass

# Initialize the environment
env = PortfolioEnv()

# Check the environment with Stable Baselines3
check_env(env, warn=True)

# Initialize the SAC model with the new environment
model = SAC("MlpPolicy", env, gamma=env.discount_factor, verbose=1)

# Train the model
model.learn(total_timesteps=100000)

# Save the trained model with regularization
model.save("NvidiaTeslaregulized")

# Run the trained model to evaluate performance
obs, _ = env.reset()
portfolio_values = [env.portfolio_value]

for _ in range(env.steps - env.current_step):  # Adjust the loop to start from current_step
    action, _states = model.predict(obs)
    obs, rewards, terminated, truncated, _ = env.step(action)
    portfolio_values.append(env.portfolio_value)
    env.render()
    if terminated or truncated:
        break

# Plotting function to visualize results
plt.figure(figsize=(12, 6))
plt.plot(portfolio_values, label='Portfolio Value')
plt.legend()
plt.title('Portfolio Value Over Time with Regularization')
plt.xlabel('Time Step')
plt.ylabel('Value')
plt.show()

# Import necessary packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from stable_baselines3 import SAC
from scipy.optimize import minimize

# Heston model simulation function
def heston_simulation(S0, v0, kappa, theta, sigma, rho, r, T, dt, N):
    """
    Simulate stock prices using the Heston model.
    """
    num_steps = int(T / dt)
    prices = np.zeros((N, num_steps))
    variances = np.zeros((N, num_steps))
    prices[:, 0] = S0
    variances[:, 0] = v0

    for t in range(1, num_steps):
        dW1 = np.random.normal(0, np.sqrt(dt), size=N)
        dW2 = rho * dW1 + np.sqrt(1 - rho**2) * np.random.normal(0, np.sqrt(dt), size=N)
        variances[:, t] = variances[:, t-1] + kappa * (theta - variances[:, t-1]) * dt + sigma * np.sqrt(variances[:, t-1]) * dW2
        variances[:, t] = np.maximum(variances[:, t], 0)  # Ensure variance is non-negative
        prices[:, t] = prices[:, t-1] * np.exp((r - 0.5 * variances[:, t-1]) * dt + np.sqrt(variances[:, t-1]) * dW1)

    return prices

# Function to run a single experiment
def run_experiment(env, model, T=1.0, dt=1/252):
    # Generate data using Heston parameters for TSLA and Nvidia
        # Heston parameters for TSLA and Nvidia
    heston_params_tsla = {'S0': 14.895333290100098, 'v0': 0.5389647040148708, 'kappa': 2.9210956956390786, 'theta': 0.22837916745978026, 'sigma': 0.40481651452195677, 'rho': -0.07362857586873965, 'r': 0.01}
    heston_params_Nvidia = {'S0': 52.69083784472894, 'v0': 0.5208742572323116, 'kappa': 0.49265002435163474,
                                  'theta': 0.41686951612919876, 'sigma': 0.32390578896758004, 'rho': 0.4888657113235263,
                                  'r': 0.043}


    # Simulate price paths
    prices_tsla = heston_simulation(**heston_params_tsla, T=T, dt=dt, N=1).flatten()
    prices_Nvidia = heston_simulation(**heston_params_Nvidia, T=T, dt=dt, N=1).flatten()

    returns_df = pd.DataFrame({
        'TSLA_Returns': np.diff(prices_tsla) / prices_tsla[:-1],
        'Nvidia_Returns': np.diff(prices_Nvidia) / prices_Nvidia[:-1]
    })

    # Initialize environment and model
    obs, _ = env.reset()
    portfolio_values_sac = [env.portfolio_value]

    for _ in range(env.steps - 30):
        action, _states = model.predict(obs)
        obs, reward, terminated, truncated, _ = env.step(action)
        portfolio_values_sac.append(env.portfolio_value)
        if terminated or truncated:
            break

    # Markowitz optimization
    K = 30
    portfolio_values_markowitz = [100]  # Start with initial portfolio value

    for t in range(K, len(returns_df) - 1):
        window_returns = returns_df.iloc[t-K:t]
        mu = window_returns.mean().values
        cov_matrix = window_returns.cov().values

        # Minimize the negative Sharpe ratio
        def objective(weights):
            portfolio_return = np.dot(mu, weights)
            portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
            sharpe_ratio = portfolio_return / portfolio_std
            return -sharpe_ratio

        # Constraints and bounds
        constraints = {'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1}
        bounds = tuple((0, 1) for _ in range(len(mu)))
        init_guess = np.ones(len(mu)) / len(mu)

        result = minimize(objective, init_guess, method='SLSQP', bounds=bounds, constraints=constraints)

        if result.success:
            optimal_weights = result.x
            next_return = returns_df.iloc[t + 1].values @ optimal_weights
            new_portfolio_value = portfolio_values_markowitz[-1] * (1 + next_return)
            portfolio_values_markowitz.append(new_portfolio_value)
        else:
            portfolio_values_markowitz.append(portfolio_values_markowitz[-1])

    return portfolio_values_sac, portfolio_values_markowitz

# Run 100 experiments
num_experiments = 100
cumulative_returns_sac = []
cumulative_returns_markowitz = []

# Initialize the environment
env = PortfolioEnv()

# Load the pre-trained SAC model
model = SAC.load("NvidiaTeslaUnregulized")

for _ in range(num_experiments):
    sac_returns, markowitz_returns = run_experiment(env, model)
    cumulative_returns_sac.append(sac_returns)
    cumulative_returns_markowitz.append(markowitz_returns)

# Calculate mean cumulative returns
mean_cumulative_returns_sac = np.mean(np.array([np.interp(np.linspace(0, len(ret) - 1, 252), np.arange(len(ret)), ret) for ret in cumulative_returns_sac]), axis=0)
mean_cumulative_returns_markowitz = np.mean(np.array([np.interp(np.linspace(0, len(ret) - 1, 252), np.arange(len(ret)), ret) for ret in cumulative_returns_markowitz]), axis=0)

# Plot the results
plt.figure(figsize=(14, 8))
plt.plot(mean_cumulative_returns_sac, label='SAC Strategy', color='green')
plt.plot(mean_cumulative_returns_markowitz, label='Markowitz Strategy', color='red')
plt.title('Mean Cumulative Portfolio Returns: SAC vs Markowitz (100 Experiments)')
plt.xlabel('Time Step')
plt.ylabel('Mean Cumulative Returns')
plt.legend()
plt.grid(True)
plt.show()

# Import necessary packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from stable_baselines3 import SAC
from scipy.optimize import minimize

# Heston model simulation function
def heston_simulation(S0, v0, kappa, theta, sigma, rho, r, T, dt, N):
    """
    Simulate stock prices using the Heston model.
    """
    num_steps = int(T / dt)
    prices = np.zeros((N, num_steps))
    variances = np.zeros((N, num_steps))
    prices[:, 0] = S0
    variances[:, 0] = v0

    for t in range(1, num_steps):
        dW1 = np.random.normal(0, np.sqrt(dt), size=N)
        dW2 = rho * dW1 + np.sqrt(1 - rho**2) * np.random.normal(0, np.sqrt(dt), size=N)
        variances[:, t] = variances[:, t-1] + kappa * (theta - variances[:, t-1]) * dt + sigma * np.sqrt(variances[:, t-1]) * dW2
        variances[:, t] = np.maximum(variances[:, t], 0)  # Ensure variance is non-negative
        prices[:, t] = prices[:, t-1] * np.exp((r - 0.5 * variances[:, t-1]) * dt + np.sqrt(variances[:, t-1]) * dW1)

    return prices

# Function to run a single experiment
def run_experiment(env, model, T=1.0, dt=1/252):
    # Generate data using Heston parameters for TSLA and Nvidia
    heston_params_tsla = {'S0': 14.895333290100098, 'v0': 0.04, 'kappa': 1.5, 'theta': 0.04, 'sigma': 0.2, 'rho': -0.5, 'r': 0.01}
    heston_params_Nvidia = {'S0': 52.69083784472894, 'v0': 0.039999945005990946, 'kappa': 1.499999938146619,
                         'theta': 0.040002536011096795, 'sigma': 0.2000000361535595, 'rho': -0.5000001006619291,
                         'r': 0.009999849145866864}

    # Simulate price paths
    prices_tsla = heston_simulation(**heston_params_tsla, T=T, dt=dt, N=1).flatten()
    prices_Nvidia = heston_simulation(**heston_params_Nvidia, T=T, dt=dt, N=1).flatten()

    returns_df = pd.DataFrame({
        'TSLA_Returns': np.diff(prices_tsla) / prices_tsla[:-1],
        'Nvidia_Returns': np.diff(prices_Nvidia) / prices_Nvidia[:-1]
    })

    # Initialize environment and model
    obs, _ = env.reset()
    portfolio_values_sac = [env.portfolio_value]

    for _ in range(env.steps - 30):
        action, _states = model.predict(obs)
        obs, reward, terminated, truncated, _ = env.step(action)
        portfolio_values_sac.append(env.portfolio_value)
        if terminated or truncated:
            break

    # Markowitz optimization
    K = 30
    portfolio_values_markowitz = [100]  # Start with initial portfolio value

    for t in range(K, len(returns_df) - 1):
        window_returns = returns_df.iloc[t-K:t]
        mu = window_returns.mean().values
        cov_matrix = window_returns.cov().values

        # Minimize the negative Sharpe ratio
        def objective(weights):
            portfolio_return = np.dot(mu, weights)
            portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
            sharpe_ratio = portfolio_return / portfolio_std
            return -sharpe_ratio

        # Constraints and bounds
        constraints = {'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1}
        bounds = tuple((0, 1) for _ in range(len(mu)))
        init_guess = np.ones(len(mu)) / len(mu)

        result = minimize(objective, init_guess, method='SLSQP', bounds=bounds, constraints=constraints)

        if result.success:
            optimal_weights = result.x
            next_return = returns_df.iloc[t + 1].values @ optimal_weights
            new_portfolio_value = portfolio_values_markowitz[-1] * (1 + next_return)
            portfolio_values_markowitz.append(new_portfolio_value)
        else:
            portfolio_values_markowitz.append(portfolio_values_markowitz[-1])

    return portfolio_values_sac, portfolio_values_markowitz

# Run 100 experiments
num_experiments = 1000
cumulative_returns_sac = []
cumulative_returns_markowitz = []

# Initialize the environment
env = PortfolioEnv()

# Load the pre-trained SAC model
model = SAC.load("sac_portfolio_model_with_tau")

for _ in range(num_experiments):
    sac_returns, markowitz_returns = run_experiment(env, model)
    cumulative_returns_sac.append(sac_returns)
    cumulative_returns_markowitz.append(markowitz_returns)

# Calculate mean cumulative returns
mean_cumulative_returns_sac = np.mean(np.array([np.interp(np.linspace(0, len(ret) - 1, 252), np.arange(len(ret)), ret) for ret in cumulative_returns_sac]), axis=0)
mean_cumulative_returns_markowitz = np.mean(np.array([np.interp(np.linspace(0, len(ret) - 1, 252), np.arange(len(ret)), ret) for ret in cumulative_returns_markowitz]), axis=0)

# Plot the results
plt.figure(figsize=(14, 8))
plt.plot(mean_cumulative_returns_sac, label='SAC Strategy', color='green')
plt.plot(mean_cumulative_returns_markowitz, label='Markowitz Strategy', color='red')
plt.title('Mean Cumulative Portfolio Returns: SAC vs Markowitz (1000 Experiments)')
plt.xlabel('Time Step')
plt.ylabel('Mean Cumulative Returns')
plt.legend()
plt.grid(True)
plt.show()

!pip install gymnasium stable-baselines3[extra] numpy matplotlib pandas scipy

import numpy as np
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import SAC
from stable_baselines3.common.env_checker import check_env
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# Heston model simulation function
def heston_simulation(S0, v0, kappa, theta, sigma, rho, r, T, dt, N):
    """
    Simulate stock prices using the Heston model.
    """
    num_steps = int(T / dt)
    prices = np.zeros((N, num_steps))
    variances = np.zeros((N, num_steps))
    prices[:, 0] = S0
    variances[:, 0] = v0

    for t in range(1, num_steps):
        dW1 = np.random.normal(0, np.sqrt(dt), size=N)
        dW2 = rho * dW1 + np.sqrt(1 - rho**2) * np.random.normal(0, np.sqrt(dt), size=N)
        variances[:, t] = variances[:, t-1] + kappa * (theta - variances[:, t-1]) * dt + sigma * np.sqrt(variances[:, t-1]) * dW2
        variances[:, t] = np.maximum(variances[:, t], 0)  # Ensure variance is non-negative
        prices[:, t] = prices[:, t-1] * np.exp((r - 0.5 * variances[:, t-1]) * dt + np.sqrt(variances[:, t-1]) * dW1)

    return prices

# Define the custom environment for the RL agent
class PortfolioEnv(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, T=1.0, dt=1/252, initial_portfolio_value=100):
        super(PortfolioEnv, self).__init__()

        # Parameters
        self.T = T
        self.dt = dt
        self.steps = int(T / dt)
        self.initial_portfolio_value = initial_portfolio_value
        self.current_step = 30  # Start after collecting initial window for mean/volatility
        self.risk_free_rate = 0.045 / 252  # Daily risk-free rate approximation
        self.discount_factor = (1 - self.risk_free_rate)

        # Updated state space to match the correct observation vector length
        # State space: [current_prices (2), current_weights (3), portfolio_value (1), mean_prices (2), volatility (2), tau (1)]
        self.observation_space = spaces.Box(
            low=np.array([0, 0, 0, 0, 0, 0, -np.inf, 0, 0, 0, 0]),
            high=np.array([np.inf, np.inf, 1, 1, 1, np.inf, np.inf, np.inf, np.inf, np.inf, self.steps]),
            dtype=np.float32
        )

        # Action space: portfolio weights for TSLA and Nvidia, third weight is derived (risk-free)
        self.action_space = spaces.Box(low=0, high=1, shape=(2,), dtype=np.float32)

        # Heston parameters for TSLA and Nvidia
        self.heston_params_tsla = {'S0': 14.895333290100098, 'v0': 0.5389647040148708, 'kappa': 2.9210956956390786, 'theta': 0.22837916745978026, 'sigma': 0.40481651452195677, 'rho': -0.07362857586873965, 'r': 0.01}
        self.heston_params_Nvidia = {'S0': 52.69083784472894, 'v0': 0.5208742572323116, 'kappa': 0.49265002435163474,
                                  'theta': 0.41686951612919876, 'sigma': 0.32390578896758004, 'rho': 0.4888657113235263,
                                  'r': 0.043}

        self.reset()

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.current_step = 30
        self.portfolio_value = self.initial_portfolio_value

        # Simulate the price paths
        self.prices_tsla = heston_simulation(**self.heston_params_tsla, T=self.T, dt=self.dt, N=1).flatten()
        self.prices_Nvidia = heston_simulation(**self.heston_params_Nvidia, T=self.T, dt=self.dt, N=1).flatten()

        self.weights = np.array([1/3, 1/3, 1/3])  # Start with equal weights
        self.tau = self.steps - self.current_step  # Time to end of episode

        # Calculate initial mean and volatility for the last 30 steps
        self.mean_tsla = np.mean(self.prices_tsla[:self.current_step])
        self.mean_Nvidia = np.mean(self.prices_Nvidia[:self.current_step])
        self.volatility_tsla = np.std(self.prices_tsla[:self.current_step])
        self.volatility_Nvidia = np.std(self.prices_Nvidia[:self.current_step])

        return self._get_obs(), {}

    def _get_obs(self):
        # Correct the observation vector to match the observation space definition
        return np.array([
            self.prices_tsla[self.current_step],
            self.prices_Nvidia[self.current_step],
            *self.weights,
            self.portfolio_value,
            self.mean_tsla,
            self.mean_Nvidia,
            self.volatility_tsla,
            self.volatility_Nvidia,
            self.tau
        ], dtype=np.float32)

    def step(self, action):
        # Ensure weights sum to 1 by including the risk-free asset weight
        weights = np.append(action, 1 - np.sum(action))

        # Clip weights to ensure they are within [0, 1] range and sum to 1
        weights = np.clip(weights, 0, 1)
        weights = weights / weights.sum()

        # Update portfolio
        self.current_step += 1
        self.tau -= 1  # Decrease the time to terminal
        if self.current_step >= (self.steps - 1):
            terminated = True
            truncated = False
            reward = 0
            return self._get_obs(), reward, terminated, truncated, {}

        terminated = self.current_step >= self.steps - 1  # Stop before we run out of data
        truncated = False

        # Calculate returns
        returns_tsla = (self.prices_tsla[self.current_step] - self.prices_tsla[self.current_step - 1]) / self.prices_tsla[self.current_step - 1]
        returns_Nvidia = (self.prices_Nvidia[self.current_step] - self.prices_Nvidia[self.current_step - 1]) / self.prices_Nvidia[self.current_step - 1]
        returns_rf = self.risk_free_rate

        # Portfolio return
        portfolio_return = weights[0] * returns_tsla + weights[1] * returns_Nvidia + weights[2] * returns_rf
        self.portfolio_value *= (1 + portfolio_return)

        # Update moving averages and volatilities
        self.mean_tsla = np.mean(self.prices_tsla[self.current_step-30:self.current_step])
        self.mean_Nvidia = np.mean(self.prices_Nvidia[self.current_step-30:self.current_step])
        self.volatility_tsla = np.std(self.prices_tsla[self.current_step-30:self.current_step])
        self.volatility_Nvidia = np.std(self.prices_Nvidia[self.current_step-30:self.current_step])

        # Update state
        self.weights = weights
        state = self._get_obs()

        # Reward calculation (Sharpe Ratio)
        excess_return = portfolio_return - self.risk_free_rate
        risk = np.std([returns_tsla, returns_Nvidia, returns_rf])
        sharpe_ratio = excess_return / (risk + 1e-8)
        reward = sharpe_ratio

        return state, reward, terminated, truncated, {}

    def render(self, mode='human'):
        print(f"Step: {self.current_step}, Portfolio Value: {self.portfolio_value:.2f}, Time to End: {self.tau}")

    def close(self):
        pass

# Initialize the environment
env = PortfolioEnv()

# Check the environment with Stable Baselines3
check_env(env, warn=True)

!pip install gymnasium stable-baselines3[extra] numpy matplotlib pandas scipy
import numpy as np
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import SAC
from stable_baselines3.common.env_checker import check_env
import matplotlib.pyplot as plt
import yfinance as yf
from scipy.optimize import minimize

# Function to estimate Black-Scholes parameters using MLE formulas provided
def get_black_scholes_params(ticker):
    # Fetch historical data for 2023
    data = yf.download(ticker, start='2023-01-01', end='2023-12-31')
    prices = data['Adj Close'].values
    n = len(prices) - 1
    dt = 1 / 252  # Assuming daily data

    # Calculate log returns
    log_returns = np.log(prices[1:] / prices[:-1])

    # MLE for volatility (sigma^2)
    r_bar = np.mean(log_returns)
    sigma_hat_sq = (1 / (n * dt)) * np.sum((log_returns - r_bar) ** 2)

    # Return the parameters without the drift (mu is not needed for simulation)
    return {'S0': prices[0], 'sigma': np.sqrt(sigma_hat_sq), 'r': 0.05}  # Assume 5% risk-free rate

# Black-Scholes model simulation function
def black_scholes_simulation(S0, sigma, r, T, dt, N):
    """
    Simulate stock prices using the Black-Scholes model.
    """
    num_steps = int(T / dt)
    prices = np.zeros((N, num_steps))
    prices[:, 0] = S0

    for t in range(1, num_steps):
        dW = np.random.normal(0, np.sqrt(dt), size=N)
        prices[:, t] = prices[:, t-1] * np.exp((r - 0.5 * sigma**2) * dt + sigma * dW)

    return prices

# Define the custom environment for the RL agent
class PortfolioEnv(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, T=1.0, dt=1/252, initial_portfolio_value=100):
        super(PortfolioEnv, self).__init__()

        # Parameters
        self.T = T
        self.dt = dt
        self.steps = int(T / dt)
        self.initial_portfolio_value = initial_portfolio_value
        self.current_step = 30  # Start after collecting initial window for mean/volatility
        self.risk_free_rate = 0.045 / 252  # Daily risk-free rate approximation
        self.discount_factor = (1 - self.risk_free_rate)

        # Updated state space to match the correct observation vector length
        self.observation_space = spaces.Box(
            low=np.array([0, 0, 0, 0, 0, 0, -np.inf, 0, 0, 0, 0]),
            high=np.array([np.inf, np.inf, 1, 1, 1, np.inf, np.inf, np.inf, np.inf, np.inf, self.steps]),
            dtype=np.float32
        )

        # Action space: portfolio weights for TSLA and Nvidia, third weight is derived (risk-free)
        self.action_space = spaces.Box(low=0, high=1, shape=(2,), dtype=np.float32)

        # Fetch Black-Scholes parameters for TSLA and Nvidia using Yahoo Finance data
        self.bs_params_tsla = get_black_scholes_params('TSLA')
        self.bs_params_nvidia = get_black_scholes_params('NVDA')

        self.reset()

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.current_step = 30
        self.portfolio_value = self.initial_portfolio_value

        # Simulate the price paths using the Black-Scholes model
        self.prices_tsla = black_scholes_simulation(**self.bs_params_tsla, T=self.T, dt=self.dt, N=1).flatten()
        self.prices_Nvidia = black_scholes_simulation(**self.bs_params_nvidia, T=self.T, dt=self.dt, N=1).flatten()

        self.weights = np.array([1/3, 1/3, 1/3])  # Start with equal weights
        self.tau = self.steps - self.current_step  # Time to end of episode

        # Calculate initial mean and volatility for the last 30 steps
        self.mean_tsla = np.mean(self.prices_tsla[:self.current_step])
        self.mean_Nvidia = np.mean(self.prices_Nvidia[:self.current_step])
        self.volatility_tsla = np.std(self.prices_tsla[:self.current_step])
        self.volatility_Nvidia = np.std(self.prices_Nvidia[:self.current_step])

        return self._get_obs(), {}

    def _get_obs(self):
        # Correct the observation vector to match the observation space definition
        return np.array([
            self.prices_tsla[self.current_step],
            self.prices_Nvidia[self.current_step],
            *self.weights,
            self.portfolio_value,
            self.mean_tsla,
            self.mean_Nvidia,
            self.volatility_tsla,
            self.volatility_Nvidia,
            self.tau
        ], dtype=np.float32)

    def step(self, action):
        # Ensure weights sum to 1 by including the risk-free asset weight
        weights = np.append(action, 1 - np.sum(action))

        # Clip weights to ensure they are within [0, 1] range and sum to 1
        weights = np.clip(weights, 0, 1)
        weights = weights / weights.sum()

        # Update portfolio
        self.current_step += 1
        self.tau -= 1  # Decrease the time to terminal
        if self.current_step >= (self.steps - 1):
            terminated = True
            truncated = False
            reward = 0
            return self._get_obs(), reward, terminated, truncated, {}

        terminated = self.current_step >= self.steps - 1  # Stop before we run out of data
        truncated = False

        # Calculate returns
        returns_tsla = (self.prices_tsla[self.current_step] - self.prices_tsla[self.current_step - 1]) / self.prices_tsla[self.current_step - 1]
        returns_Nvidia = (self.prices_Nvidia[self.current_step] - self.prices_Nvidia[self.current_step - 1]) / self.prices_Nvidia[self.current_step - 1]
        returns_rf = self.risk_free_rate

        # Portfolio return
        portfolio_return = weights[0] * returns_tsla + weights[1] * returns_Nvidia + weights[2] * returns_rf
        self.portfolio_value *= (1 + portfolio_return)

        # Update moving averages and volatilities
        self.mean_tsla = np.mean(self.prices_tsla[self.current_step-30:self.current_step])
        self.mean_Nvidia = np.mean(self.prices_Nvidia[self.current_step-30:self.current_step])
        self.volatility_tsla = np.std(self.prices_tsla[self.current_step-30:self.current_step])
        self.volatility_Nvidia = np.std(self.prices_Nvidia[self.current_step-30:self.current_step])

        # Update state
        self.weights = weights
        state = self._get_obs()

        # Reward calculation (Sharpe Ratio)
        reward = portfolio_return

        return state, reward, terminated, truncated, {}

    def render(self, mode='human'):
        print(f"Step: {self.current_step}, Portfolio Value: {self.portfolio_value:.2f}, Time to End: {self.tau}")

    def close(self):
        pass

# Initialize the environment
env = PortfolioEnv()

# Check the environment with Stable Baselines3
check_env(env, warn=True)

# Initialize the SAC model with the new environment
model = SAC("MlpPolicy", env, gamma=env.discount_factor, verbose=1)

# Train the model
model.learn(total_timesteps=100000)

# Save the trained model
model.save("NvidiaTeslaUnregulized_BlackScholes")

# Run the trained model to evaluate performance
obs, _ = env.reset()
portfolio_values = [env.portfolio_value]

for _ in range(env.steps - env.current_step):  # Adjust the loop to start from current_step
    action, _states = model.predict(obs)
    obs, rewards, terminated, truncated, _ = env.step(action)
    portfolio_values.append(env.portfolio_value)
    env.render()
    if terminated or truncated:
        break

# Plotting function to visualize results
plt.figure(figsize=(12, 6))
plt.plot(portfolio_values, label='Portfolio Value')
plt.legend()
plt.title('Portfolio Value Over Time (Black-Scholes)')
plt.xlabel('Time Step')
plt.ylabel('Value')
plt.show()

!pip install gymnasium stable-baselines3[extra] numpy matplotlib pandas scipy
import numpy as np
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import SAC
from stable_baselines3.common.env_checker import check_env
import matplotlib.pyplot as plt
import yfinance as yf
from scipy.optimize import minimize

# Function to estimate Black-Scholes parameters using MLE formulas provided
def get_black_scholes_params(ticker):
    # Fetch historical data for 2023
    data = yf.download(ticker, start='2023-01-01', end='2023-12-31')
    prices = data['Adj Close'].values
    n = len(prices) - 1
    dt = 1 / 252  # Assuming daily data

    # Calculate log returns
    log_returns = np.log(prices[1:] / prices[:-1])

    # MLE for volatility (sigma^2)
    r_bar = np.mean(log_returns)
    sigma_hat_sq = (1 / (n * dt)) * np.sum((log_returns - r_bar) ** 2)

    # Return the parameters without the drift (mu is not needed for simulation)
    return {'S0': prices[0], 'sigma': np.sqrt(sigma_hat_sq), 'r': 0.05}  # Assume 5% risk-free rate

# Black-Scholes model simulation function
def black_scholes_simulation(S0, sigma, r, T, dt, N):
    """
    Simulate stock prices using the Black-Scholes model.
    """
    num_steps = int(T / dt)
    prices = np.zeros((N, num_steps))
    prices[:, 0] = S0

    for t in range(1, num_steps):
        dW = np.random.normal(0, np.sqrt(dt), size=N)
        prices[:, t] = prices[:, t-1] * np.exp((r - 0.5 * sigma**2) * dt + sigma * dW)

    return prices

# Define the custom environment for the RL agent
class PortfolioEnv(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, T=1.0, dt=1/252, initial_portfolio_value=100):
        super(PortfolioEnv, self).__init__()

        # Parameters
        self.T = T
        self.dt = dt
        self.steps = int(T / dt)
        self.initial_portfolio_value = initial_portfolio_value
        self.current_step = 30  # Start after collecting initial window for mean/volatility
        self.risk_free_rate = 0.045 / 252  # Daily risk-free rate approximation
        self.discount_factor = (1 - self.risk_free_rate)
        self.Sigma = np.array([[0.000523, 0.000275], [0.000275, 0.000878]])  # Covariance matrix for Mahalanobis norm
        self.delta_r = 0.1091841496941713  # Radius delta_r

        # Updated state space to match the correct observation vector length
        self.observation_space = spaces.Box(
            low=np.array([0, 0, 0, 0, 0, 0, -np.inf, 0, 0, 0, 0]),
            high=np.array([np.inf, np.inf, 1, 1, 1, np.inf, np.inf, np.inf, np.inf, np.inf, self.steps]),
            dtype=np.float32
        )

        # Action space: portfolio weights for TSLA and Nvidia, third weight is derived (risk-free)
        self.action_space = spaces.Box(low=0, high=1, shape=(2,), dtype=np.float32)

        # Fetch Black-Scholes parameters for TSLA and Nvidia using Yahoo Finance data
        self.bs_params_tsla = get_black_scholes_params('TSLA')
        self.bs_params_nvidia = get_black_scholes_params('NVDA')

        self.reset()

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.current_step = 30
        self.portfolio_value = self.initial_portfolio_value

        # Simulate the price paths using the Black-Scholes model
        self.prices_tsla = black_scholes_simulation(**self.bs_params_tsla, T=self.T, dt=self.dt, N=1).flatten()
        self.prices_Nvidia = black_scholes_simulation(**self.bs_params_nvidia, T=self.T, dt=self.dt, N=1).flatten()

        self.weights = np.array([1/3, 1/3, 1/3])  # Start with equal weights
        self.tau = self.steps - self.current_step  # Time to end of episode

        # Calculate initial mean and volatility for the last 30 steps
        self.mean_tsla = np.mean(self.prices_tsla[:self.current_step])
        self.mean_Nvidia = np.mean(self.prices_Nvidia[:self.current_step])
        self.volatility_tsla = np.std(self.prices_tsla[:self.current_step])
        self.volatility_Nvidia = np.std(self.prices_Nvidia[:self.current_step])

        return self._get_obs(), {}

    def _get_obs(self):
        # Correct the observation vector to match the observation space definition
        return np.array([
            self.prices_tsla[self.current_step],
            self.prices_Nvidia[self.current_step],
            *self.weights,
            self.portfolio_value,
            self.mean_tsla,
            self.mean_Nvidia,
            self.volatility_tsla,
            self.volatility_Nvidia,
            self.tau
        ], dtype=np.float32)
    def mahalanobis_norm(self, action):
        """
        Compute the Mahalanobis norm of the policy (action) using the covariance matrix Sigma.
        """
        action = np.array(action).reshape(-1, 1)
        norm = np.sqrt(action.T @ np.linalg.inv(self.Sigma) @ action)
        return norm[0][0]

    def step(self, action):
        # Ensure weights sum to 1 by including the risk-free asset weight
        weights = np.append(action, 1 - np.sum(action))

        # Clip weights to ensure they are within [0, 1] range and sum to 1
        weights = np.clip(weights, 0, 1)
        weights = weights / weights.sum()

        # Update portfolio
        self.current_step += 1
        self.tau -= 1  # Decrease the time to terminal
        if self.current_step >= (self.steps - 1):
            terminated = True
            truncated = False
            reward = 0
            return self._get_obs(), reward, terminated, truncated, {}

        terminated = self.current_step >= self.steps - 1  # Stop before we run out of data
        truncated = False

        # Calculate returns
        returns_tsla = (self.prices_tsla[self.current_step] - self.prices_tsla[self.current_step - 1]) / self.prices_tsla[self.current_step - 1]
        returns_Nvidia = (self.prices_Nvidia[self.current_step] - self.prices_Nvidia[self.current_step - 1]) / self.prices_Nvidia[self.current_step - 1]
        returns_rf = self.risk_free_rate

        # Portfolio return
        portfolio_return = weights[0] * returns_tsla + weights[1] * returns_Nvidia + weights[2] * returns_rf
        self.portfolio_value *= (1 + portfolio_return)

        # Update moving averages and volatilities
        self.mean_tsla = np.mean(self.prices_tsla[self.current_step-30:self.current_step])
        self.mean_Nvidia = np.mean(self.prices_Nvidia[self.current_step-30:self.current_step])
        self.volatility_tsla = np.std(self.prices_tsla[self.current_step-30:self.current_step])
        self.volatility_Nvidia = np.std(self.prices_Nvidia[self.current_step-30:self.current_step])

        # Update state
        self.weights = weights
        state = self._get_obs()

        # Reward calculation (Sharpe Ratio)
        pi_s_norm = self.mahalanobis_norm(action)
        penalty = -np.sqrt(self.delta_r) * pi_s_norm

        reward = portfolio_return + penalty

        return state, reward, terminated, truncated, {}

    def render(self, mode='human'):
        print(f"Step: {self.current_step}, Portfolio Value: {self.portfolio_value:.2f}, Time to End: {self.tau}")

    def close(self):
        pass

# Initialize the environment
env = PortfolioEnv()

# Check the environment with Stable Baselines3
check_env(env, warn=True)

# Initialize the SAC model with the new environment
model = SAC("MlpPolicy", env, gamma=env.discount_factor, verbose=1)

# Train the model
model.learn(total_timesteps=100000)

# Save the trained model
model.save("NvidiaTeslaregulized_BlackScholes")

# Run the trained model to evaluate performance
obs, _ = env.reset()
portfolio_values = [env.portfolio_value]

for _ in range(env.steps - env.current_step):  # Adjust the loop to start from current_step
    action, _states = model.predict(obs)
    obs, rewards, terminated, truncated, _ = env.step(action)
    portfolio_values.append(env.portfolio_value)
    env.render()
    if terminated or truncated:
        break

# Plotting function to visualize results
plt.figure(figsize=(12, 6))
plt.plot(portfolio_values, label='Portfolio Value')
plt.legend()
plt.title('Portfolio Value Over Time (Black-Scholes)')
plt.xlabel('Time Step')
plt.ylabel('Value')
plt.show()

import numpy as np
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import SAC
from stable_baselines3.common.env_checker import check_env
import matplotlib.pyplot as plt
import yfinance as yf
from scipy.optimize import minimize
import pandas as pd
# Define the custom environment for the RL agent
class PortfolioEnv(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, T=1.0, dt=1/252, initial_portfolio_value=100):
        super(PortfolioEnv, self).__init__()

        # Parameters
        self.T = T
        self.dt = dt
        self.steps = int(T / dt)
        self.initial_portfolio_value = initial_portfolio_value
        self.current_step = 30  # Start after collecting initial window for mean/volatility
        self.risk_free_rate = 0.000152  # Adjusted risk-free rate
        self.discount_factor = (1 - self.risk_free_rate)
        self.portfolio_value = self.initial_portfolio_value

        # Updated state space to include portfolio value
        self.observation_space = spaces.Box(
            low=np.array([0, 0, 0, 0, 0, -np.inf, 0, 0, 0, 0, 0]),
            high=np.array([np.inf, np.inf, 1, 1, np.inf, np.inf, np.inf, np.inf, np.inf, np.inf, self.steps]),
            dtype=np.float32
        )

        # Action space: portfolio weights for TSLA and Nvidia, third weight is derived (risk-free)
        self.action_space = spaces.Box(low=0, high=1, shape=(2,), dtype=np.float32)

        self.reset()

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.current_step = 30
        self.portfolio_value = self.initial_portfolio_value

        # Placeholder price paths (these would be replaced by real data in real experiments)
        self.prices_tsla = np.ones(self.steps) * 100  # Replace this with real data later
        self.prices_nvidia = np.ones(self.steps) * 100

        self.weights = np.array([1/3, 1/3, 1/3])  # Start with equal weights
        self.tau = self.steps - self.current_step  # Time to end of episode

        # Calculate initial mean and volatility for the last 30 steps
        self.mean_tsla = np.mean(self.prices_tsla[:self.current_step])
        self.mean_nvidia = np.mean(self.prices_nvidia[:self.current_step])
        self.volatility_tsla = np.std(self.prices_tsla[:self.current_step])
        self.volatility_nvidia = np.std(self.prices_nvidia[:self.current_step])

        return self._get_obs(), {}

    def _get_obs(self):
        return np.array([
            self.prices_tsla[self.current_step],   # TSLA current price
            self.prices_nvidia[self.current_step], # Nvidia current price
            *self.weights,                        # Portfolio weights
            self.portfolio_value,                 # Portfolio value
            self.mean_tsla,                       # Running mean of TSLA
            self.mean_nvidia,                     # Running mean of Nvidia
            self.volatility_tsla,                 # Running volatility of TSLA
            self.volatility_nvidia,               # Running volatility of Nvidia
            self.tau                              # Time to end
        ], dtype=np.float32)

    def step(self, action):
        # Ensure weights sum to 1 by including the risk-free asset weight
        weights = np.append(action, 1 - np.sum(action))

        # Clip weights to ensure they are within [0, 1] range and sum to 1
        weights = np.clip(weights, 0, 1)
        weights = weights / weights.sum()

        # Update portfolio
        self.current_step += 1
        if self.current_step >= (self.steps - 1):
            terminated = True
            truncated = False
            reward = 0
            return self._get_obs(), reward, terminated, truncated, {}

        terminated = self.current_step >= self.steps - 1  # Stop before we run out of data
        truncated = False

        # Calculate returns
        returns_tsla = (self.prices_tsla[self.current_step] - self.prices_tsla[self.current_step - 1]) / self.prices_tsla[self.current_step - 1]
        returns_nvidia = (self.prices_nvidia[self.current_step] - self.prices_nvidia[self.current_step - 1]) / self.prices_nvidia[self.current_step - 1]
        returns_rf = self.risk_free_rate

        # Portfolio return
        portfolio_return = weights[0] * returns_tsla + weights[1] * returns_nvidia + weights[2] * returns_rf
        self.portfolio_value *= (1 + portfolio_return)

        # Update moving averages and volatilities
        self.mean_tsla = np.mean(self.prices_tsla[self.current_step-30:self.current_step])
        self.mean_nvidia = np.mean(self.prices_nvidia[self.current_step-30:self.current_step])
        self.volatility_tsla = np.std(self.prices_tsla[self.current_step-30:self.current_step])
        self.volatility_nvidia = np.std(self.prices_nvidia[self.current_step-30:self.current_step])

        # Update state
        self.weights = weights
        state = self._get_obs()

        # Reward calculation
        reward = portfolio_return

        return state, reward, terminated, truncated, {}

    def render(self, mode='human'):
        print(f"Step: {self.current_step}, Portfolio Value: {self.portfolio_value:.2f}, Time to End: {self.tau}")

    def close(self):
        pass

# Initialize the environment
env = PortfolioEnv()

# Check the environment with Stable Baselines3
check_env(env, warn=True)

# Fetch the real Tesla and Nvidia stock data from Yahoo Finance for 2024
def get_real_data():
    tesla_data = yf.download('TSLA', start='2024-01-01', end='2024-09-09')
    nvidia_data = yf.download('NVDA', start='2024-01-01', end='2024-09-09')
    return tesla_data['Adj Close'].values, nvidia_data['Adj Close'].values

# Adjust the state representation to include portfolio_value
def get_state_representation(prices_tsla, prices_nvidia, current_step, portfolio_value, window=30):
    mean_tsla = np.mean(prices_tsla[current_step-window:current_step])
    mean_nvidia = np.mean(prices_nvidia[current_step-window:current_step])
    volatility_tsla = np.std(prices_tsla[current_step-window:current_step])
    volatility_nvidia = np.std(prices_nvidia[current_step-window:current_step])
    tau = len(prices_tsla) - current_step
    return np.array([
        prices_tsla[current_step],   # TSLA current price
        prices_nvidia[current_step], # Nvidia current price
        1/3, 1/3, 1/3,              # Start with equal portfolio weights
        portfolio_value,             # Current portfolio value
        mean_tsla,                   # Running mean of TSLA
        mean_nvidia,                 # Running mean of Nvidia
        volatility_tsla,             # Running volatility of TSLA
        volatility_nvidia,           # Running volatility of Nvidia
        tau                          # Time to today (end of data)
    ], dtype=np.float32)

# Function to run one experiment with real data for both models and Markowitz strategy
def run_experiment_real_data(env, model, regularized_model, delta_values, tesla_prices, nvidia_prices, T=1.0):
    returns_df = pd.DataFrame({
        'TSLA_Returns': np.diff(tesla_prices) / tesla_prices[:-1],
        'Nvidia_Returns': np.diff(nvidia_prices) / nvidia_prices[:-1]
    })

    def run_sac_model(model):
        obs, _ = env.reset()
        portfolio_values = [env.portfolio_value]
        current_step = 30  # Start on the 30th day
        portfolio_value = env.initial_portfolio_value

        for _ in range(current_step, len(tesla_prices) - 1):
            obs = get_state_representation(tesla_prices, nvidia_prices, current_step, portfolio_value)
            action, _states = model.predict(obs)
            obs, reward, terminated, truncated, _ = env.step(action)
            portfolio_value = env.portfolio_value
            portfolio_values.append(portfolio_value)
            current_step += 1
            if terminated or truncated:
                break
        return portfolio_values

    # Run SAC (unregularized) several times
    sac_portfolios_all = []
    for i in range(5):
        model.set_random_seed(i)  # Different seed each time
        sac_portfolios = run_sac_model(model)
        sac_portfolios_all.append(sac_portfolios)

    # Run Regularized SAC
    reg_sac_portfolios_all = []
    for i in range(5):
        regularized_model.set_random_seed(i)  # Different seed each time
        reg_sac_portfolios = run_sac_model(regularized_model)
        reg_sac_portfolios_all.append(reg_sac_portfolios)

    # Average the portfolio values over the 5 runs
    mean_sac_returns = np.mean(sac_portfolios_all, axis=0)
    mean_reg_sac_returns = np.mean(reg_sac_portfolios_all, axis=0)

    # Walk-forward Markowitz optimization with regularization
    K = 30
    portfolio_values_markowitz = [100]  # Start with initial portfolio value
    portfolio_values_reg_markowitz = {delta: [100] for delta in delta_values}

    for t in range(K, len(returns_df) - 1):
        window_returns = returns_df.iloc[t-K:t]
        mu = window_returns.mean().values
        cov_matrix = window_returns.cov().values

        # Unregularized Markowitz optimization
        def objective(weights):
            portfolio_return = np.dot(mu, weights)
            portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
            sharpe_ratio = portfolio_return / portfolio_std
            return -sharpe_ratio

        # Constraints and bounds
        constraints = {'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1}
        bounds = tuple((0, 1) for _ in range(len(mu)))
        init_guess = np.ones(len(mu)) / len(mu)

        result = minimize(objective, init_guess, method='SLSQP', bounds=bounds, constraints=constraints)

        if result.success:
            optimal_weights = result.x
            next_return = returns_df.iloc[t + 1].values @ optimal_weights
            new_portfolio_value = portfolio_values_markowitz[-1] * (1 + next_return)
            portfolio_values_markowitz.append(new_portfolio_value)
        else:
            portfolio_values_markowitz.append(portfolio_values_markowitz[-1])

        # Regularized Markowitz optimization for each delta value
        for delta in delta_values:
            def regularized_objective(weights):
                portfolio_return = np.dot(mu, weights)
                portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
                regularization_term = delta * np.dot(weights.T, np.dot(cov_matrix, weights))
                return -(portfolio_return - np.sqrt(delta) * portfolio_std - regularization_term)

            result_reg = minimize(regularized_objective, init_guess, method='SLSQP', bounds=bounds, constraints=constraints)

            if result_reg.success:
                optimal_weights_reg = result_reg.x
                next_return_reg = returns_df.iloc[t + 1].values @ optimal_weights_reg
                new_portfolio_value_reg = portfolio_values_reg_markowitz[delta][-1] * (1 + next_return_reg)
                portfolio_values_reg_markowitz[delta].append(new_portfolio_value_reg)
            else:
                portfolio_values_reg_markowitz[delta].append(portfolio_values_reg_markowitz[delta][-1])

    return mean_sac_returns, mean_reg_sac_returns, portfolio_values_markowitz, portfolio_values_reg_markowitz

# Run 100 experiments with real data
delta_values = [0.1, 0.5, 1, 10]  # Different delta values for regularization
tesla_prices, nvidia_prices = get_real_data()

# Initialize the environment
env = PortfolioEnv()

# Load the pre-trained SAC models (unregularized and regularized)
model_unreg = SAC.load("NvidiaTeslaUnregulized_BlackScholes")
model_reg = SAC.load("NvidiaTeslaregulized_BlackScholes")

mean_sac_returns, mean_reg_sac_returns, markowitz_returns, reg_markowitz_returns = run_experiment_real_data(
    env, model_unreg, model_reg, delta_values, tesla_prices, nvidia_prices)

# Plot the results for cumulative returns
plt.figure(figsize=(14, 8))
plt.plot(mean_sac_returns, label='SAC Strategy (Regularized)', color='green')
plt.plot(mean_reg_sac_returns, label='SAC Strategy (Regularized)', color='blue')
plt.plot(markowitz_returns, label='Markowitz Strategy (Unregularized)', color='red')
for delta in delta_values:
    plt.plot(reg_markowitz_returns[delta], label=f'Regularized Markowitz (Delta = {delta})')

plt.title('Cumulative Portfolio Returns: SAC vs Regularized SAC vs Markowitz (Real Data Jan-Sep 2024)')
plt.xlabel('Time Step')
plt.ylabel('Cumulative Returns')
plt.legend()
plt.grid(True)
plt.show()